{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install langchain openai faiss-cpu PyPDF2 gradio\n"
      ],
      "metadata": {
        "id": "3jMX9pUlQfZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.chat_models import ChatGooglePalm\n",
        "\n",
        "# Set your OpenAI API key here or export it in your environment\n",
        "chat=ChatGooglePalm(api_key=\"\")\n",
        "\n",
        "# Load and split the PDF document\n",
        "loader = PyPDFLoader(\"/content/A_Deep_Learning-Based_Ensemble_Framework_for_Robus.pdf\")\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Create vector store from documents\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "# Initialize LLM\n",
        "\n",
        "# Setup conversational retrieval chain\n",
        "qa = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever())\n",
        "\n",
        "# Keep chat history in memory\n",
        "chat_history = []\n",
        "\n",
        "def chatbot(user_input):\n",
        "    global chat_history\n",
        "    result = qa({\"question\": user_input, \"chat_history\": chat_history})\n",
        "    chat_history.append((user_input, result[\"answer\"]))\n",
        "    return result[\"answer\"]\n",
        "\n",
        "# Build Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=chatbot,\n",
        "    inputs=gr.Textbox(lines=4, placeholder=\"Ask me anything about the document...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Document QA Chatbot\",\n",
        "    description=\"Ask questions about your PDF document and get answers powered by OpenAI and LangChain.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch()\n"
      ],
      "metadata": {
        "id": "NccLhTQel3ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "id": "MZpg_IUJl3e4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pypdf"
      ],
      "metadata": {
        "id": "07A_wJ2Rl3cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bcdVefPTl3Yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SdmwqMqTl3V7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}